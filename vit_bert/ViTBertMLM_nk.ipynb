{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViTBertMLM_nk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCJXZl25YvOK"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers sentencepiece flax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oMaEJXUvxAl"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000001532.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "text = \"A highway with a black car on it\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9jkzjp-wRAl"
      },
      "source": [
        "from transformers import ViTFeatureExtractor, FlaxViTModel, BertTokenizerFast, FlaxBertModel, BertConfig, ViTConfig\n",
        "from transformers.models.vit.modeling_flax_vit import FlaxViTModule\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from jax import lax, random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvC7kbXp5_ne"
      },
      "source": [
        "vit_config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "bert_config = BertConfig.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uwsjqg6kUL2"
      },
      "source": [
        "## Flax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwkmXO-7pe2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02428496-0771-4b34-8d9d-ce02adf73d25"
      },
      "source": [
        "# configuration = ViTConfig(hidden_size=1024)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model_vit = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "vit_inputs = feature_extractor(images=image, return_tensors=\"jax\")\n",
        "encoder_outputs = model_vit(**vit_inputs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKyAGaFZx0ip"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJP03zq1xT9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e11d3b-4ada-4384-ad85-93d9e02fed8e"
      },
      "source": [
        "encoder_outputs['last_hidden_state'].shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 197, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTcsI6MzqAhS",
        "outputId": "7a648511-1fbb-44da-d394-5fc36adbd8f5"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
        "model_bert = FlaxBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "bert_inputs = tokenizer(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of FlaxBertModel were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: {('pooler', 'dense', 'bias'), ('pooler', 'dense', 'kernel')}\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96G2bqv5JwD3",
        "outputId": "c97e07a5-525e-4ac4-b545-205ed84ac1f6"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "AutoConfig.for_model(**bert_config.to_dict())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.8.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpdLviuc6KWd"
      },
      "source": [
        "import copy\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class ViTBertConfig(PretrainedConfig):\n",
        "    model_type = \"vit-bert\"\n",
        "    is_composition = True\n",
        "\n",
        "    def __init__(self, bert_config_dict, vit_config_dict, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        if bert_config_dict is None:\n",
        "            raise ValueError(\"`bert_config_dict` can not be `None`.\")\n",
        "\n",
        "        if vit_config_dict is None:\n",
        "            raise ValueError(\"`vit_config_dict` can not be `None`.\")\n",
        "\n",
        "        self.bert_config = BertConfig(**bert_config_dict)\n",
        "\n",
        "        self.vit_config = ViTConfig(**vit_config_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def from_bert_vit_configs(cls, bert_config: PretrainedConfig, vit_config: PretrainedConfig, **kwargs):\n",
        "        r\"\"\"\n",
        "        Instantiate a :class:`HybridCLIPConfig` (or a derived class) from text model configuration and\n",
        "        vision model configuration.\n",
        "        Returns:\n",
        "            :class:`HybridCLIPConfig`: An instance of a configuration object\n",
        "        \"\"\"\n",
        "\n",
        "        return cls(bert_config_dict=bert_config.to_dict(), vit_config_dict=vit_config.to_dict(), **kwargs)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serializes this instance to a Python dictionary. Override the default\n",
        "        :meth:`~transformers.PretrainedConfig.to_dict`.\n",
        "        Returns:\n",
        "            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n",
        "        \"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        output[\"bert_config\"] = self.bert_config.to_dict()\n",
        "        output[\"vit_config\"] = self.vit_config.to_dict()\n",
        "        output[\"model_type\"] = self.__class__.model_type\n",
        "        return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJEPOSxgBzFF"
      },
      "source": [
        "class FlaxViTBertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    config: ViTBertConfig\n",
        "    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n",
        "\n",
        "    def setup(self):\n",
        "        bert_config = self.config.bert_config\n",
        "        vit_config = self.config.vit_config\n",
        "\n",
        "        self.word_embeddings = nn.Embed(\n",
        "            bert_config.vocab_size,\n",
        "            bert_config.hidden_size,\n",
        "            embedding_init=jax.nn.initializers.normal(stddev=bert_config.initializer_range),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.position_embeddings = nn.Embed(\n",
        "            bert_config.max_position_embeddings,\n",
        "            bert_config.hidden_size,\n",
        "            embedding_init=jax.nn.initializers.normal(stddev=bert_config.initializer_range),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.token_type_embeddings = nn.Embed(\n",
        "            bert_config.type_vocab_size,\n",
        "            bert_config.hidden_size,\n",
        "            embedding_init=jax.nn.initializers.normal(stddev=bert_config.initializer_range),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "        self.vit_module = FlaxViTModule(vit_config, dtype=self.dtype)\n",
        "        self.visual_projection = nn.Dense(bert_config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(bert_config.initializer_range, self.dtype))\n",
        "\n",
        "        self.visual_position_embeddings = nn.Embed(\n",
        "            bert_config.max_position_embeddings,\n",
        "            bert_config.hidden_size,\n",
        "            embedding_init=jax.nn.initializers.normal(stddev=bert_config.initializer_range),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.visual_token_type_embeddings = nn.Embed(\n",
        "            bert_config.type_vocab_size,\n",
        "            bert_config.hidden_size,\n",
        "            embedding_init=jax.nn.initializers.normal(stddev=bert_config.initializer_range),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(epsilon=bert_config.layer_norm_eps, dtype=self.dtype)\n",
        "        self.dropout = nn.Dropout(rate=bert_config.hidden_dropout_prob)\n",
        "        \n",
        "    def __call__(self, input_ids, token_type_ids, position_ids, pixel_values, visual_token_type_ids, visual_position_ids, deterministic: bool = True):\n",
        "        # Embed\n",
        "        inputs_embeds = self.word_embeddings(input_ids.astype(\"i4\"))\n",
        "        position_embeds = self.position_embeddings(position_ids.astype(\"i4\"))\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids.astype(\"i4\"))\n",
        "\n",
        "        # Sum all embeddings\n",
        "        word_embeddings = inputs_embeds + token_type_embeddings + position_embeds\n",
        "\n",
        "        # Visual Embed\n",
        "        visual_inputs_embeds = self.vit_module(pixel_values=pixel_values)[0]\n",
        "        visual_inputs_embeds = self.visual_projection(visual_inputs_embeds)\n",
        "        visual_token_type_embeddings = self.visual_token_type_embeddings(visual_token_type_ids.astype(\"i4\"))\n",
        "        visual_position_embeds = self.visual_position_embeddings(visual_position_ids.astype(\"i4\"))\n",
        "\n",
        "        # Sum all visual embeddings\n",
        "        visual_embeddings = visual_inputs_embeds + visual_token_type_embeddings + visual_position_embeds\n",
        "\n",
        "        # Concat\n",
        "        hidden_states = jnp.concatenate((word_embeddings, visual_embeddings),axis=1)\n",
        "\n",
        "        # Layer Norm\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n",
        "        return hidden_states"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0T5UUtEqoW7"
      },
      "source": [
        "input_ids = jnp.array([bert_inputs[\"input_ids\"]], dtype=jnp.int32)\n",
        "token_type_ids = jnp.array([bert_inputs['token_type_ids']], dtype=jnp.int32)\n",
        "attention_mask = jnp.array([bert_inputs['attention_mask']], dtype=jnp.int32)\n",
        "position_ids = jnp.arange(0, input_ids.shape[1], dtype=jnp.int32).reshape(1, -1)\n",
        "pixel_values = vit_inputs['pixel_values']\n",
        "visual_attention_mask = jnp.ones(encoder_outputs.last_hidden_state.shape[:-1], dtype=jnp.int32)\n",
        "visual_token_type_ids = jnp.ones(encoder_outputs.last_hidden_state.shape[:-1], dtype=jnp.int32)\n",
        "visual_position_ids = jnp.zeros(encoder_outputs.last_hidden_state.shape[:-1], dtype=jnp.int32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf8kTLGSjsWq"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTn_WZKFh-8H",
        "outputId": "083a026c-0881-484f-d198-306900ea80e6"
      },
      "source": [
        "attention_mask.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjqbqAwiBRx",
        "outputId": "68ec36b1-148e-414e-b3f0-fcd6209dc381"
      },
      "source": [
        "visual_attention_mask.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 197)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7km9glyiC2m",
        "outputId": "f911ea38-3c5b-44c7-faee-9b13081f287f"
      },
      "source": [
        "jnp.concatenate((attention_mask, visual_attention_mask), axis=1).shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 207)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwc2RYX293wW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddb79e4-bd17-4330-aa6d-f1a285247e7c"
      },
      "source": [
        "vit_bert_config = ViTBertConfig.from_bert_vit_configs(bert_config, vit_config)\n",
        "vit_bert_config"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTBertConfig {\n",
              "  \"bert_config\": {\n",
              "    \"_name_or_path\": \"\",\n",
              "    \"add_cross_attention\": false,\n",
              "    \"architectures\": [\n",
              "      \"BertForMaskedLM\"\n",
              "    ],\n",
              "    \"attention_probs_dropout_prob\": 0.1,\n",
              "    \"bad_words_ids\": null,\n",
              "    \"bos_token_id\": null,\n",
              "    \"chunk_size_feed_forward\": 0,\n",
              "    \"decoder_start_token_id\": null,\n",
              "    \"diversity_penalty\": 0.0,\n",
              "    \"do_sample\": false,\n",
              "    \"early_stopping\": false,\n",
              "    \"encoder_no_repeat_ngram_size\": 0,\n",
              "    \"eos_token_id\": null,\n",
              "    \"finetuning_task\": null,\n",
              "    \"forced_bos_token_id\": null,\n",
              "    \"forced_eos_token_id\": null,\n",
              "    \"gradient_checkpointing\": false,\n",
              "    \"hidden_act\": \"gelu\",\n",
              "    \"hidden_dropout_prob\": 0.1,\n",
              "    \"hidden_size\": 768,\n",
              "    \"id2label\": {\n",
              "      \"0\": \"LABEL_0\",\n",
              "      \"1\": \"LABEL_1\"\n",
              "    },\n",
              "    \"initializer_range\": 0.02,\n",
              "    \"intermediate_size\": 3072,\n",
              "    \"is_decoder\": false,\n",
              "    \"is_encoder_decoder\": false,\n",
              "    \"label2id\": {\n",
              "      \"LABEL_0\": 0,\n",
              "      \"LABEL_1\": 1\n",
              "    },\n",
              "    \"layer_norm_eps\": 1e-12,\n",
              "    \"length_penalty\": 1.0,\n",
              "    \"max_length\": 20,\n",
              "    \"max_position_embeddings\": 512,\n",
              "    \"min_length\": 0,\n",
              "    \"model_type\": \"bert\",\n",
              "    \"no_repeat_ngram_size\": 0,\n",
              "    \"num_attention_heads\": 12,\n",
              "    \"num_beam_groups\": 1,\n",
              "    \"num_beams\": 1,\n",
              "    \"num_hidden_layers\": 12,\n",
              "    \"num_return_sequences\": 1,\n",
              "    \"output_attentions\": false,\n",
              "    \"output_hidden_states\": false,\n",
              "    \"output_scores\": false,\n",
              "    \"pad_token_id\": 0,\n",
              "    \"position_embedding_type\": \"absolute\",\n",
              "    \"prefix\": null,\n",
              "    \"problem_type\": null,\n",
              "    \"pruned_heads\": {},\n",
              "    \"remove_invalid_values\": false,\n",
              "    \"repetition_penalty\": 1.0,\n",
              "    \"return_dict\": true,\n",
              "    \"return_dict_in_generate\": false,\n",
              "    \"sep_token_id\": null,\n",
              "    \"task_specific_params\": null,\n",
              "    \"temperature\": 1.0,\n",
              "    \"tie_encoder_decoder\": false,\n",
              "    \"tie_word_embeddings\": true,\n",
              "    \"tokenizer_class\": null,\n",
              "    \"top_k\": 50,\n",
              "    \"top_p\": 1.0,\n",
              "    \"torchscript\": false,\n",
              "    \"transformers_version\": \"4.8.2\",\n",
              "    \"type_vocab_size\": 2,\n",
              "    \"use_bfloat16\": false,\n",
              "    \"use_cache\": true,\n",
              "    \"vocab_size\": 30522\n",
              "  },\n",
              "  \"model_type\": \"vit-bert\",\n",
              "  \"transformers_version\": null,\n",
              "  \"vit_config\": {\n",
              "    \"_name_or_path\": \"\",\n",
              "    \"add_cross_attention\": false,\n",
              "    \"architectures\": [\n",
              "      \"ViTModel\"\n",
              "    ],\n",
              "    \"attention_probs_dropout_prob\": 0.0,\n",
              "    \"bad_words_ids\": null,\n",
              "    \"bos_token_id\": null,\n",
              "    \"chunk_size_feed_forward\": 0,\n",
              "    \"decoder_start_token_id\": null,\n",
              "    \"diversity_penalty\": 0.0,\n",
              "    \"do_sample\": false,\n",
              "    \"early_stopping\": false,\n",
              "    \"encoder_no_repeat_ngram_size\": 0,\n",
              "    \"eos_token_id\": null,\n",
              "    \"finetuning_task\": null,\n",
              "    \"forced_bos_token_id\": null,\n",
              "    \"forced_eos_token_id\": null,\n",
              "    \"hidden_act\": \"gelu\",\n",
              "    \"hidden_dropout_prob\": 0.0,\n",
              "    \"hidden_size\": 768,\n",
              "    \"id2label\": {\n",
              "      \"0\": \"LABEL_0\",\n",
              "      \"1\": \"LABEL_1\"\n",
              "    },\n",
              "    \"image_size\": 224,\n",
              "    \"initializer_range\": 0.02,\n",
              "    \"intermediate_size\": 3072,\n",
              "    \"is_decoder\": false,\n",
              "    \"is_encoder_decoder\": false,\n",
              "    \"label2id\": {\n",
              "      \"LABEL_0\": 0,\n",
              "      \"LABEL_1\": 1\n",
              "    },\n",
              "    \"layer_norm_eps\": 1e-12,\n",
              "    \"length_penalty\": 1.0,\n",
              "    \"max_length\": 20,\n",
              "    \"min_length\": 0,\n",
              "    \"model_type\": \"vit\",\n",
              "    \"no_repeat_ngram_size\": 0,\n",
              "    \"num_attention_heads\": 12,\n",
              "    \"num_beam_groups\": 1,\n",
              "    \"num_beams\": 1,\n",
              "    \"num_channels\": 3,\n",
              "    \"num_hidden_layers\": 12,\n",
              "    \"num_return_sequences\": 1,\n",
              "    \"output_attentions\": false,\n",
              "    \"output_hidden_states\": false,\n",
              "    \"output_scores\": false,\n",
              "    \"pad_token_id\": null,\n",
              "    \"patch_size\": 16,\n",
              "    \"prefix\": null,\n",
              "    \"problem_type\": null,\n",
              "    \"pruned_heads\": {},\n",
              "    \"remove_invalid_values\": false,\n",
              "    \"repetition_penalty\": 1.0,\n",
              "    \"return_dict\": true,\n",
              "    \"return_dict_in_generate\": false,\n",
              "    \"sep_token_id\": null,\n",
              "    \"task_specific_params\": null,\n",
              "    \"temperature\": 1.0,\n",
              "    \"tie_encoder_decoder\": false,\n",
              "    \"tie_word_embeddings\": true,\n",
              "    \"tokenizer_class\": null,\n",
              "    \"top_k\": 50,\n",
              "    \"top_p\": 1.0,\n",
              "    \"torchscript\": false,\n",
              "    \"transformers_version\": \"4.8.2\",\n",
              "    \"use_bfloat16\": false\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtW9JTAwpMYC"
      },
      "source": [
        "# flax_embedding_layer = FlaxViTBertEmbeddings(vit_bert_config)\n",
        "# key = random.PRNGKey(0)\n",
        "# params = flax_embedding_layer.init(key, input_ids, token_type_ids, position_ids, pixel_values, visual_token_type_ids, visual_position_ids)\n",
        "# jax.tree_map(lambda x: x.shape, params)\n",
        "# flax_embedding_layer.apply(params, input_ids, token_type_ids, position_ids, pixel_values, visual_token_type_ids, visual_position_ids).shape"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93rZUAur2tVi"
      },
      "source": [
        "from transformers.models.bert.modeling_flax_bert import FlaxPreTrainedModel, FlaxBertEncoder, FlaxBertPooler, FlaxBaseModelOutputWithPooling, FlaxBertPreTrainedModel\n",
        "from typing import Tuple, Optional\n",
        "from flax.core.frozen_dict import FrozenDict"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE99Ogljxtkl"
      },
      "source": [
        "class FlaxViTBertModule(nn.Module):\n",
        "    config: ViTBertConfig\n",
        "    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n",
        "    add_pooling_layer: bool = True\n",
        "\n",
        "    def setup(self):\n",
        "        self.embeddings = FlaxViTBertEmbeddings(self.config, dtype=self.dtype)\n",
        "        self.encoder = FlaxBertEncoder(self.config.bert_config, dtype=self.dtype)\n",
        "        self.pooler = FlaxBertPooler(self.config.bert_config, dtype=self.dtype)\n",
        "        print(self.embeddings)\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        position_ids,\n",
        "        pixel_values,\n",
        "        visual_attention_mask,\n",
        "        visual_token_type_ids, \n",
        "        visual_position_ids,\n",
        "        deterministic = True,\n",
        "        output_attentions: bool = False,\n",
        "        output_hidden_states: bool = False,\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        hidden_states = self.embeddings(\n",
        "            input_ids, token_type_ids, position_ids, pixel_values, visual_token_type_ids, visual_position_ids, deterministic=deterministic\n",
        "        )\n",
        "\n",
        "        combined_attention_mask = jnp.concatenate((attention_mask, visual_attention_mask), axis=1)\n",
        "        deterministic=True\n",
        "        outputs = self.encoder(\n",
        "            hidden_states,\n",
        "            combined_attention_mask,\n",
        "            deterministic=True,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = outputs[0]\n",
        "        pooled = self.pooler(hidden_states) if self.add_pooling_layer else None\n",
        "\n",
        "        if not return_dict:\n",
        "            # if pooled is None, don't return it\n",
        "            if pooled is None:\n",
        "                return (hidden_states,) + outputs[1:]\n",
        "            return (hidden_states, pooled) + outputs[1:]\n",
        "\n",
        "        return FlaxBaseModelOutputWithPooling(\n",
        "            last_hidden_state=hidden_states,\n",
        "            pooler_output=pooled,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcW4lm0M5iyg"
      },
      "source": [
        "class FlaxViTBertModel(FlaxPreTrainedModel):\n",
        "    config_class: ViTBertConfig\n",
        "    module_class = FlaxViTBertModule\n",
        "\n",
        "    def __init__(\n",
        "        self, config: ViTBertConfig, input_shape: Tuple = None, seed: int = 0, dtype: jnp.dtype = jnp.float32, **kwargs\n",
        "    ):\n",
        "\n",
        "        if input_shape is None:\n",
        "            input_shape = ((1, 1), (1, config.vit_config.image_size, config.vit_config.image_size, 3), (1, 197))\n",
        "\n",
        "        module = self.module_class(config=config, dtype=dtype, **kwargs)\n",
        "        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype)\n",
        "\n",
        "    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple) -> FrozenDict:\n",
        "        # init input tensors\n",
        "        textual_input_shape = input_shape[0]\n",
        "        input_ids = jnp.zeros(textual_input_shape, dtype=\"i4\")\n",
        "        token_type_ids = jnp.zeros_like(input_ids)\n",
        "        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), textual_input_shape)\n",
        "        attention_mask = jnp.ones_like(input_ids)\n",
        "\n",
        "        pixel_values = jax.random.normal(rng, input_shape[1])\n",
        "        visual_attention_mask = jnp.ones(input_shape[2]) # TODO: Fix this\n",
        "        visual_token_type_ids = jnp.ones(input_shape[2]) # TODO: Fix this\n",
        "        visual_position_ids = jnp.broadcast_to(jnp.zeros(jnp.atleast_2d(input_ids).shape[-1]), input_shape[2]) # TODO: Fix this\n",
        "\n",
        "        params_rng, dropout_rng = jax.random.split(rng)\n",
        "        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n",
        "\n",
        "        return self.module.init(rngs, input_ids, attention_mask, token_type_ids, position_ids, pixel_values,\n",
        "        visual_attention_mask,\n",
        "        visual_token_type_ids, \n",
        "        visual_position_ids, return_dict=False)[\n",
        "            \"params\"\n",
        "        ]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        pixel_values=None,\n",
        "        visual_attention_mask=None,\n",
        "        visual_token_type_ids=None, \n",
        "        visual_position_ids=None,\n",
        "        \n",
        "        params: dict = None,\n",
        "        dropout_rng: jax.random.PRNGKey = None,\n",
        "        train: bool = False,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.bert_config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.bert_config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.bert_config.return_dict\n",
        "\n",
        "\n",
        "        pixel_values = jnp.transpose(pixel_values, (0, 2, 3, 1))\n",
        "\n",
        "        # init input tensors if not passed\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = jnp.zeros_like(input_ids)\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = jnp.ones_like(input_ids)\n",
        "\n",
        "        if visual_token_type_ids is None:\n",
        "            visual_token_type_ids = jnp.ones(input_ids.shape) # TODO: Fix this.\n",
        "\n",
        "        if visual_position_ids is None:\n",
        "            visual_position_ids = jnp.broadcast_to(jnp.atleast_2d(input_ids).shape[-1],input_ids.shape) # TODO: Fix this.\n",
        "\n",
        "        if visual_attention_mask is None:\n",
        "            visual_attention_mask = jnp.ones(input_ids.shape) # TODO: Fix this.\n",
        "\n",
        "        # Handle any PRNG if needed\n",
        "        rngs = {}\n",
        "        if dropout_rng is not None:\n",
        "            rngs[\"dropout\"] = dropout_rng\n",
        "\n",
        "        return self.module.apply(\n",
        "            {\"params\": params or self.params},\n",
        "            jnp.array(input_ids, dtype=\"i4\"),\n",
        "            jnp.array(attention_mask, dtype=\"i4\"),\n",
        "            jnp.array(token_type_ids, dtype=\"i4\"),\n",
        "            jnp.array(position_ids, dtype=\"i4\"),\n",
        "            jnp.array(pixel_values, dtype=jnp.float32),\n",
        "            jnp.array(visual_attention_mask, dtype=\"i4\"),\n",
        "            jnp.array(visual_token_type_ids, dtype=\"i4\"),\n",
        "            jnp.array(visual_position_ids, dtype=\"i4\"),\n",
        "            not train,\n",
        "            output_attentions,\n",
        "            output_hidden_states,\n",
        "            return_dict,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_bert_vit_pretrained(\n",
        "        cls,\n",
        "        bert_model_name_or_path: str = None,\n",
        "        vit_model_name_or_path: str = None,\n",
        "        *model_args,\n",
        "        **kwargs,\n",
        "    ) -> FlaxPreTrainedModel:\n",
        "\n",
        "        kwargs_bert = {\n",
        "            argument[len(\"bert_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"text_\")\n",
        "        }\n",
        "\n",
        "        kwargs_vit = {\n",
        "            argument[len(\"vit_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"vision_\")\n",
        "        }\n",
        "\n",
        "        # remove text, vision kwargs from kwargs\n",
        "        for key in kwargs_bert.keys():\n",
        "            del kwargs[\"bert_\" + key]\n",
        "        for key in kwargs_vit.keys():\n",
        "            del kwargs[\"vit_\" + key]\n",
        "\n",
        "        # Load and initialize the text and vision model\n",
        "        bert_model = kwargs_bert.pop(\"model\", None)\n",
        "        if bert_model is None:\n",
        "            assert (\n",
        "                bert_model_name_or_path is not None\n",
        "            ), \"If `model` is not defined as an argument, a `bert_model_name_or_path` has to be defined\"\n",
        "            from transformers import FlaxBertModel\n",
        "\n",
        "            if \"config\" not in kwargs_bert:\n",
        "                from transformers import BertConfig\n",
        "\n",
        "                bert_config = BertConfig.from_pretrained(bert_model_name_or_path)\n",
        "                kwargs_bert[\"config\"] = bert_config\n",
        "\n",
        "            bert_model = FlaxBertModel.from_pretrained(\n",
        "                bert_model_name_or_path, *model_args, from_pt=True, **kwargs_bert\n",
        "            )\n",
        "\n",
        "        vit_model = kwargs_vit.pop(\"model\", None)\n",
        "        if vit_model is None:\n",
        "            assert (\n",
        "                vit_model_name_or_path is not None\n",
        "            ), \"If `model` is not defined as an argument, a `vit_model_name_or_path` has to be defined\"\n",
        "            from transformers import FlaxViTModel\n",
        "\n",
        "            if \"config\" not in kwargs_vit:\n",
        "                from transformers import ViTConfig\n",
        "\n",
        "                vit_config = ViTConfig.from_pretrained(vit_model_name_or_path)\n",
        "                kwargs_vit[\"config\"] = vit_config\n",
        "\n",
        "            vit_model = FlaxViTModel.from_pretrained(vit_model_name_or_path, *model_args, **kwargs_vit)\n",
        "\n",
        "        # instantiate config with corresponding kwargs\n",
        "        dtype = kwargs.pop(\"dtype\", jnp.float32)\n",
        "        config = ViTBertConfig.from_bert_vit_configs(bert_model.config, vit_model.config, **kwargs)\n",
        "\n",
        "        # init model\n",
        "        model = cls(config, *model_args, dtype=dtype, **kwargs)\n",
        "\n",
        "        for key in model.params.keys():\n",
        "            if key != \"embeddings\":\n",
        "                model.params[key] = bert_model.params[key]\n",
        "            else:\n",
        "                model.params[\"embeddings\"][\"vit_module\"] = vit_model.params\n",
        "                for sub_key in bert_model.params[key]:\n",
        "                    model.params[key][sub_key] = bert_model.params[key][sub_key]\n",
        "\n",
        "        return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js7iOAWK2_5f",
        "outputId": "3a74f554-cc75-435a-8856-9e3f3a1625fe"
      },
      "source": [
        "flax_model = FlaxViTBertModel.from_bert_vit_pretrained('bert-base-uncased', 'google/vit-base-patch16-224-in21k', seed=0, dtype=jnp.float32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing FlaxBertModel: {('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'seq_relationship', 'bias'), ('cls', 'seq_relationship', 'kernel'), ('cls', 'predictions', 'decoder', 'kernel'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'weight'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'bias')}\n",
            "- This IS expected if you are initializing FlaxBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing FlaxBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FlaxViTBertEmbeddings(\n",
            "    # attributes\n",
            "    config = ViTBertConfig {\n",
            "      \"bert_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"BertForMaskedLM\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.1,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"gradient_checkpointing\": false,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.1,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"max_position_embeddings\": 512,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"bert\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": 0,\n",
            "        \"position_embedding_type\": \"absolute\",\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"type_vocab_size\": 2,\n",
            "        \"use_bfloat16\": false,\n",
            "        \"use_cache\": true,\n",
            "        \"vocab_size\": 30522\n",
            "      },\n",
            "      \"model_type\": \"vit-bert\",\n",
            "      \"seed\": 0,\n",
            "      \"transformers_version\": null,\n",
            "      \"vit_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"ViTModel\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.0,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.0,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"image_size\": 224,\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"vit\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_channels\": 3,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": null,\n",
            "        \"patch_size\": 16,\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"use_bfloat16\": false\n",
            "      }\n",
            "    }\n",
            "    \n",
            "    dtype = float32\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SR_G-5fm0rH",
        "outputId": "e35d047a-1485-4099-ce81-1f45dcda916a"
      },
      "source": [
        "outputs = flax_model(input_ids, attention_mask,token_type_ids, position_ids, pixel_values, visual_attention_mask, visual_token_type_ids, visual_position_ids, output_hidden_states=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FlaxViTBertEmbeddings(\n",
            "    # attributes\n",
            "    config = ViTBertConfig {\n",
            "      \"bert_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"BertForMaskedLM\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.1,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"gradient_checkpointing\": false,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.1,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"max_position_embeddings\": 512,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"bert\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": 0,\n",
            "        \"position_embedding_type\": \"absolute\",\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"type_vocab_size\": 2,\n",
            "        \"use_bfloat16\": false,\n",
            "        \"use_cache\": true,\n",
            "        \"vocab_size\": 30522\n",
            "      },\n",
            "      \"model_type\": \"vit-bert\",\n",
            "      \"seed\": 0,\n",
            "      \"transformers_version\": null,\n",
            "      \"vit_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"ViTModel\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.0,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.0,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"image_size\": 224,\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"vit\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_channels\": 3,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": null,\n",
            "        \"patch_size\": 16,\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"use_bfloat16\": false\n",
            "      }\n",
            "    }\n",
            "    \n",
            "    dtype = float32\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBLV1_-OVmaR",
        "outputId": "a3d9da6e-ae8f-4c56-80ec-05d5bd3e0d1f"
      },
      "source": [
        "outputs.keys()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlhaJRRdj-r-"
      },
      "source": [
        "from transformers.models.bert.modeling_flax_bert import FlaxBertOnlyMLMHead\n",
        "\n",
        "class FlaxViTBertForMaskedLMModule(nn.Module):\n",
        "    config: ViTBertConfig\n",
        "    dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "    def setup(self):\n",
        "        self.vitbert = FlaxViTBertModule(config=self.config, add_pooling_layer=False, dtype=self.dtype)\n",
        "        self.cls = FlaxBertOnlyMLMHead(config=self.config.bert_config, dtype=self.dtype)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        pixel_values=None,\n",
        "        visual_attention_mask=None,\n",
        "        visual_token_type_ids=None, \n",
        "        visual_position_ids=None,\n",
        "        params: dict = None,\n",
        "        dropout_rng: jax.random.PRNGKey = None,\n",
        "        train: bool = False,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "\n",
        "        # Model\n",
        "        outputs = self.vitbert(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                token_type_ids,\n",
        "                position_ids,\n",
        "                pixel_values,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids, \n",
        "                visual_position_ids,\n",
        "                deterministic=True,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        if self.config.bert_config.tie_word_embeddings:\n",
        "            shared_embedding = self.vitbert.variables[\"params\"][\"embeddings\"][\"word_embeddings\"][\"embedding\"]\n",
        "        else:\n",
        "            shared_embedding = None\n",
        "\n",
        "        # Compute the prediction scores\n",
        "        logits = self.cls(hidden_states, shared_embedding=shared_embedding)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (logits,) + outputs[1:]\n",
        "\n",
        "        return FlaxMaskedLMOutput(\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmnCMfyh5ODm"
      },
      "source": [
        "class FlaxViTBertForMaskedLM(FlaxViTBertModel):\n",
        "    module_class = FlaxViTBertForMaskedLMModule\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GknvebGrLuQ4",
        "outputId": "8e5917b1-9c3a-4adb-eefc-9d31b928562b"
      },
      "source": [
        "flax_vitbert_mlm = FlaxViTBertForMaskedLM(vit_bert_config)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FlaxViTBertEmbeddings(\n",
            "    # attributes\n",
            "    config = ViTBertConfig {\n",
            "      \"bert_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"BertForMaskedLM\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.1,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"gradient_checkpointing\": false,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.1,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"max_position_embeddings\": 512,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"bert\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": 0,\n",
            "        \"position_embedding_type\": \"absolute\",\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"type_vocab_size\": 2,\n",
            "        \"use_bfloat16\": false,\n",
            "        \"use_cache\": true,\n",
            "        \"vocab_size\": 30522\n",
            "      },\n",
            "      \"model_type\": \"vit-bert\",\n",
            "      \"transformers_version\": null,\n",
            "      \"vit_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"ViTModel\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.0,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.0,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"image_size\": 224,\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"vit\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_channels\": 3,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": null,\n",
            "        \"patch_size\": 16,\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"use_bfloat16\": false\n",
            "      }\n",
            "    }\n",
            "    \n",
            "    dtype = float32\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6dkjwr4ABTf",
        "outputId": "ec6337bf-02d4-4dd1-890e-fe32ea187e1a"
      },
      "source": [
        "outputs_vit = flax_vitbert_mlm (input_ids=input_ids,pixel_values=pixel_values,\n",
        "                                 visual_attention_mask = visual_attention_mask,\\\n",
        "                                 visual_token_type_ids=visual_token_type_ids,\n",
        "                                 visual_position_ids=visual_position_ids)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FlaxViTBertEmbeddings(\n",
            "    # attributes\n",
            "    config = ViTBertConfig {\n",
            "      \"bert_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"BertForMaskedLM\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.1,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"gradient_checkpointing\": false,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.1,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"max_position_embeddings\": 512,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"bert\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": 0,\n",
            "        \"position_embedding_type\": \"absolute\",\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"type_vocab_size\": 2,\n",
            "        \"use_bfloat16\": false,\n",
            "        \"use_cache\": true,\n",
            "        \"vocab_size\": 30522\n",
            "      },\n",
            "      \"model_type\": \"vit-bert\",\n",
            "      \"transformers_version\": null,\n",
            "      \"vit_config\": {\n",
            "        \"_name_or_path\": \"\",\n",
            "        \"add_cross_attention\": false,\n",
            "        \"architectures\": [\n",
            "          \"ViTModel\"\n",
            "        ],\n",
            "        \"attention_probs_dropout_prob\": 0.0,\n",
            "        \"bad_words_ids\": null,\n",
            "        \"bos_token_id\": null,\n",
            "        \"chunk_size_feed_forward\": 0,\n",
            "        \"decoder_start_token_id\": null,\n",
            "        \"diversity_penalty\": 0.0,\n",
            "        \"do_sample\": false,\n",
            "        \"early_stopping\": false,\n",
            "        \"encoder_no_repeat_ngram_size\": 0,\n",
            "        \"eos_token_id\": null,\n",
            "        \"finetuning_task\": null,\n",
            "        \"forced_bos_token_id\": null,\n",
            "        \"forced_eos_token_id\": null,\n",
            "        \"hidden_act\": \"gelu\",\n",
            "        \"hidden_dropout_prob\": 0.0,\n",
            "        \"hidden_size\": 768,\n",
            "        \"id2label\": {\n",
            "          \"0\": \"LABEL_0\",\n",
            "          \"1\": \"LABEL_1\"\n",
            "        },\n",
            "        \"image_size\": 224,\n",
            "        \"initializer_range\": 0.02,\n",
            "        \"intermediate_size\": 3072,\n",
            "        \"is_decoder\": false,\n",
            "        \"is_encoder_decoder\": false,\n",
            "        \"label2id\": {\n",
            "          \"LABEL_0\": 0,\n",
            "          \"LABEL_1\": 1\n",
            "        },\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"length_penalty\": 1.0,\n",
            "        \"max_length\": 20,\n",
            "        \"min_length\": 0,\n",
            "        \"model_type\": \"vit\",\n",
            "        \"no_repeat_ngram_size\": 0,\n",
            "        \"num_attention_heads\": 12,\n",
            "        \"num_beam_groups\": 1,\n",
            "        \"num_beams\": 1,\n",
            "        \"num_channels\": 3,\n",
            "        \"num_hidden_layers\": 12,\n",
            "        \"num_return_sequences\": 1,\n",
            "        \"output_attentions\": false,\n",
            "        \"output_hidden_states\": false,\n",
            "        \"output_scores\": false,\n",
            "        \"pad_token_id\": null,\n",
            "        \"patch_size\": 16,\n",
            "        \"prefix\": null,\n",
            "        \"problem_type\": null,\n",
            "        \"pruned_heads\": {},\n",
            "        \"remove_invalid_values\": false,\n",
            "        \"repetition_penalty\": 1.0,\n",
            "        \"return_dict\": true,\n",
            "        \"return_dict_in_generate\": false,\n",
            "        \"sep_token_id\": null,\n",
            "        \"task_specific_params\": null,\n",
            "        \"temperature\": 1.0,\n",
            "        \"tie_encoder_decoder\": false,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"tokenizer_class\": null,\n",
            "        \"top_k\": 50,\n",
            "        \"top_p\": 1.0,\n",
            "        \"torchscript\": false,\n",
            "        \"transformers_version\": \"4.8.2\",\n",
            "        \"use_bfloat16\": false\n",
            "      }\n",
            "    }\n",
            "    \n",
            "    dtype = float32\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br9Q0v7aPSy4",
        "outputId": "0ba89a30-ad05-4c01-fcab-e9265dcd1e56"
      },
      "source": [
        "outputs_vit[0].shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 207, 30522)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs34_WucuB-o"
      },
      "source": [
        "class FlaxViTMaskedLM(FlaxPreTrainedModel):\n",
        "    config_class: ViTBertConfig\n",
        "    module_class = FlaxViTBertForMaskedLMModule\n",
        "    dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.vitbert = FlaxViTBertModel(config=self.config, add_pooling_layer=False,dtype=self.dtype)\n",
        "        self.cls = FlaxBertOnlyMLMHead(config=self.config.bert_config, dtype=self.dtype)\n",
        "\n",
        "        self.init_weights()\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        pixel_values=None,\n",
        "        visual_attention_mask=None,\n",
        "        visual_token_type_ids=None, \n",
        "        visual_position_ids=None,\n",
        "        params: dict = None,\n",
        "        dropout_rng: jax.random.PRNGKey = None,\n",
        "        train: bool = False,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    ):\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.bert_config.return_dict\n",
        "        pixel_values = jnp.transpose(pixel_values, (0, 2, 3, 1))\n",
        "\n",
        "        outputs = self.vitbert(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                token_type_ids,\n",
        "                position_ids,\n",
        "                pixel_values,\n",
        "                visual_attention_mask,\n",
        "                visual_token_type_ids, \n",
        "                visual_position_ids,\n",
        "                deterministic=deterministic,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.bert_config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "\n",
        "        return FlaxMaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 76,
      "outputs": []
    }
  ]
}